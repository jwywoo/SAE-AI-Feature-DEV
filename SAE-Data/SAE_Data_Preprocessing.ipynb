{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setting & Installation"
      ],
      "metadata": {
        "id": "C5KBUa2ygBKQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RnKxTo5NUMQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f715a529-7979-49a9-e582-0f745fe4f3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "TX22ycduk6rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Load"
      ],
      "metadata": {
        "id": "0TzEvm_Xf-Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "base_path = \"/content/drive/MyDrive/Profect SAE/test_data\"\n",
        "input_file = os.path.join(base_path, \"test_page_data.csv\")\n",
        "output_file = os.path.join(base_path, \"test_page_data_preprocessed.csv\")\n",
        "\n",
        "if os.path.exists(input_file):\n",
        "  print(\"input: good to go\")\n",
        "else:\n",
        "  print(\"input: not good\")\n",
        "\n",
        "if os.path.exists(output_file):\n",
        "  print(\"output: good to go\")\n",
        "else:\n",
        "  print(\"output: not good\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL2RJ2rdax1w",
        "outputId": "9b32cf24-1af2-45f1-c254-024f4cad394f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: good to go\n",
            "output: not good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "whGzwMCvgGKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(input_file)"
      ],
      "metadata": {
        "id": "yrVcpeoRjcpN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Cleaning"
      ],
      "metadata": {
        "id": "c51jtgrygKdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_cleaning(text):\n",
        "  if isinstance(text, str):\n",
        "    # lower case\n",
        "    text = text.lower()\n",
        "    # empty space removal\n",
        "    text = text.strip()\n",
        "    # multiple empty space removal\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # special character removal\n",
        "    text = re.sub(r'[^\\w\\s.,!?$%&@-]', '', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "kD3QdsLrgKPf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Cleaning\n",
        "df['cleaned_innerText'] = df['innerText'].apply(text_cleaning)\n",
        "\n",
        "# Other preprocessing tactics....\n",
        "\n",
        "# Saving\n",
        "df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "ZYsL4VMrf9uX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "fuFcpKYWjsJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def cleaning_result_comparison(df):\n",
        "  tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "  avg_legnth_decrease = 0\n",
        "  avg_token_decrease = 0\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    before = str(row['innerText'])\n",
        "    after = str(row['cleaned_innerText'])\n",
        "\n",
        "    before_length = len(before)\n",
        "    after_length = len(after)\n",
        "\n",
        "    before_tokens = len(tokenizer.encode(before))\n",
        "    after_tokens = len(tokenizer.encode(after))\n",
        "\n",
        "    avg_legnth_decrease += (before_length - after_length)\n",
        "    avg_token_decrease += (before_tokens - after_tokens)\n",
        "\n",
        "    print(f\"Row {index + 1}\")\n",
        "    print(f\" - Original Text Length: {before_length}, Cleaned Text Length: {after_length}\")\n",
        "    print(f\" - Original Text Tokens: {before_tokens}, Cleaned Text Tokens: {after_tokens}\")\n",
        "    print(\"---\")\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Result\")\n",
        "  print(f\"Number of length decreased after cleaning: {avg_legnth_decrease/len(df)}\")\n",
        "  print(f\"Number of tokens decreased after cleaning: {avg_token_decrease/len(df)}\")"
      ],
      "metadata": {
        "id": "wwu6VC2rkTTM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaning_result_comparison(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tXbgJtKmPYj",
        "outputId": "e6821230-93f4-4abb-9b32-d72cb3a7972e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 1\n",
            " - Original Text Length: 4383, Cleaned Text Length: 4208\n",
            " - Original Text Tokens: 4674, Cleaned Text Tokens: 4471\n",
            "---\n",
            "Row 2\n",
            " - Original Text Length: 7123, Cleaned Text Length: 6635\n",
            " - Original Text Tokens: 7423, Cleaned Text Tokens: 6926\n",
            "---\n",
            "Row 3\n",
            " - Original Text Length: 2215, Cleaned Text Length: 2080\n",
            " - Original Text Tokens: 2095, Cleaned Text Tokens: 1864\n",
            "---\n",
            "Row 4\n",
            " - Original Text Length: 1012, Cleaned Text Length: 932\n",
            " - Original Text Tokens: 945, Cleaned Text Tokens: 838\n",
            "---\n",
            "Row 5\n",
            " - Original Text Length: 930, Cleaned Text Length: 882\n",
            " - Original Text Tokens: 888, Cleaned Text Tokens: 796\n",
            "---\n",
            "\n",
            "Result\n",
            "Number of length decreased after cleaning: 185.2\n",
            "Number of tokens decreased after cleaning: 226.0\n"
          ]
        }
      ]
    }
  ]
}